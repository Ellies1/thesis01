# import subprocess

# vm_ip = "192.168.166.3"
# vm_user = "cloud0_zsong"
# vm_ssh_key = "/home/zsong/.ssh/id_rsa_continuum"
# vm_data_dir = "/tpcds-data"
# dataset_dir = f"{vm_data_dir}/dataset_tpcds_100G"

# def run(command, desc, exit_on_fail=True, shell=False):
#     print(f"ğŸ”§ {desc}...")
#     try:
#         subprocess.run(command, check=True, shell=shell)
#         print(f"âœ… {desc} æˆåŠŸ\n")
#     except subprocess.CalledProcessError as e:
#         print(f"âŒ {desc} å¤±è´¥: {e}\n")
#         if exit_on_fail:
#             exit(1)

# # Step 1: æ£€æŸ¥ VM ä¸Šæ˜¯å¦å·²æœ‰æ•°æ®
# check_dataset_cmd = f'ssh {vm_user}@{vm_ip} -i {vm_ssh_key} "test -d {dataset_dir} && ls -lh {dataset_dir} || echo NOT_FOUND"'
# print("ğŸ” æ£€æŸ¥ VM æ˜¯å¦å·²æœ‰ dataset_tpcds_100G æ•°æ®ç›®å½•...")
# check_output = subprocess.getoutput(check_dataset_cmd)
# if "NOT_FOUND" not in check_output:
#     print("âš ï¸ å‘ç°å·²æœ‰æ•°æ®ï¼Œæ˜¯å¦éœ€è¦åˆ é™¤ï¼Ÿä½ å¯ä»¥æ‰‹åŠ¨ ssh è¿› VM æ¸…ç†ï¼š")
#     print(f"   ssh {vm_user}@{vm_ip} -i {vm_ssh_key}")
#     print(f"   sudo rm -rf {dataset_dir}\n")
# else:
#     print("âœ… æ•°æ®ç›®å½•å°šä¸å­˜åœ¨ï¼Œå¯ä»¥ç»§ç»­ã€‚\n")

# # Step 2: åˆ›å»º VM æŒ‚è½½ç›®å½•
# mkdir_cmd = [
#     "ssh", f"{vm_user}@{vm_ip}",
#     "-i", vm_ssh_key,
#     f"sudo mkdir -p {vm_data_dir} && sudo chmod -R 777 {vm_data_dir}"
# ]
# run(mkdir_cmd, f"åˆ›å»º VM ä¸Šçš„ç›®å½• {vm_data_dir}")

# # Step 3: æ‰§è¡Œ spark-submit
# print("ğŸš€ æ­£åœ¨æäº¤ Spark parquet generator ä»»åŠ¡...\n")
# spark_submit_cmd = [
#     "~/continuum/spark-3.4.4-bin-hadoop3/bin/spark-submit",
#     "--class", "ParquetGenerator",
#     "--master", "k8s://https://192.168.166.2:6443",
#     "--deploy-mode", "cluster",
#     "--conf", "spark.kubernetes.container.image=elliesgood00/tpcds-image:v9",
#     "--conf", "spark.kubernetes.authenticate.driver.serviceAccountName=spark",
#     "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.mount.path=/tpcds-data",
#     "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.options.path=/tpcds-data",
#     "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.mount.path=/tpcds-data",
#     "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.options.path=/tpcds-data",
#     "--conf", "spark.executor.memory=6g",
#     "--conf", "spark.executor.memoryOverhead=2g",
#     "--conf", "spark.driver.memory=4g",
#     "--conf", "spark.driver.memoryOverhead=1g",
#     "--jars", "local:///opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar",
#     "local:///opt/tpcds/parquet-data-generator_2.12-1.0.jar",
#     "datagen", "/tpcds-data", "/opt/tpcds-kit/tools", "5"
# ]
# spark_submit_cmd[0] = spark_submit_cmd[0].replace("~", subprocess.getoutput("echo $HOME"))

# run(spark_submit_cmd, "æäº¤ Spark ä»»åŠ¡")

# # Step 4: å†æ¬¡æ£€æŸ¥æ˜¯å¦ç”Ÿæˆæ•°æ®
# print("ğŸ” æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦å­˜åœ¨...")
# verify_cmd = f'ssh {vm_user}@{vm_ip} -i {vm_ssh_key} "du -sh {dataset_dir} || echo ç”Ÿæˆå¤±è´¥"'
# subprocess.run(verify_cmd, shell=True)

import subprocess

# === ===
scale_factor = 5  

# === ===
vm_ip = "192.168.166.3"
vm_user = "cloud0_zsong"
vm_ssh_key = "/home/zsong/.ssh/id_rsa_continuum"
vm_data_dir = "/tpcds-data"
dataset_dir = f"{vm_data_dir}/dataset_tpcds_{scale_factor}G"

def run(command, desc, exit_on_fail=True, shell=False):
    print(f"ğŸ”§ {desc}...")
    try:
        subprocess.run(command, check=True, shell=shell)
        print(f"âœ… {desc} æˆåŠŸ\n")
    except subprocess.CalledProcessError as e:
        print(f"âŒ {desc} å¤±è´¥: {e}\n")
        if exit_on_fail:
            exit(1)

# Step 1: æ£€æŸ¥ VM ä¸Šæ˜¯å¦å·²æœ‰æ•°æ®
check_dataset_cmd = f'ssh {vm_user}@{vm_ip} -i {vm_ssh_key} "test -d {dataset_dir} && ls -lh {dataset_dir} || echo NOT_FOUND"'
print(f"ğŸ” æ£€æŸ¥ VM æ˜¯å¦å·²æœ‰ dataset_tpcds_{scale_factor}G æ•°æ®ç›®å½•...")
check_output = subprocess.getoutput(check_dataset_cmd)
if "NOT_FOUND" not in check_output:
    print("âš ï¸ å‘ç°å·²æœ‰æ•°æ®ï¼Œæ˜¯å¦éœ€è¦åˆ é™¤ï¼Ÿä½ å¯ä»¥æ‰‹åŠ¨ ssh è¿› VM æ¸…ç†ï¼š")
    print(f"   ssh {vm_user}@{vm_ip} -i {vm_ssh_key}")
    print(f"   sudo rm -rf {dataset_dir}\n")
else:
    print("âœ… æ•°æ®ç›®å½•å°šä¸å­˜åœ¨ï¼Œå¯ä»¥ç»§ç»­ã€‚\n")

# Step 2: åˆ›å»º VM æŒ‚è½½ç›®å½•
mkdir_cmd = [
    "ssh", f"{vm_user}@{vm_ip}",
    "-i", vm_ssh_key,
    f"sudo mkdir -p {vm_data_dir} && sudo chmod -R 777 {vm_data_dir}"
]
run(mkdir_cmd, f"åˆ›å»º VM ä¸Šçš„ç›®å½• {vm_data_dir}")

# Step 3: æ‰§è¡Œ spark-submit
print("ğŸš€ æ­£åœ¨æäº¤ Spark parquet generator ä»»åŠ¡...\n")
spark_submit_cmd = [
    "~/continuum/spark-3.4.4-bin-hadoop3/bin/spark-submit",
    "--class", "ParquetGenerator",
    "--master", "k8s://https://192.168.166.2:6443",
    "--deploy-mode", "cluster",
    "--conf", "spark.kubernetes.container.image=elliesgood00/tpcds-image:v9",
    "--conf", "spark.kubernetes.authenticate.driver.serviceAccountName=spark",
    "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.mount.path=/tpcds-data",
    "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.options.path=/tpcds-data",
    "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.mount.path=/tpcds-data",
    "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.options.path=/tpcds-data",
    "--conf", "spark.executor.memory=6g",
    "--conf", "spark.executor.memoryOverhead=2g",
    "--conf", "spark.driver.memory=4g",
    "--conf", "spark.driver.memoryOverhead=1g",
    "--jars", "local:///opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar",
    "local:///opt/tpcds/parquet-data-generator_2.12-1.0.jar",
    "datagen", vm_data_dir, "/opt/tpcds-kit/tools", str(scale_factor)
]
spark_submit_cmd[0] = spark_submit_cmd[0].replace("~", subprocess.getoutput("echo $HOME"))

run(spark_submit_cmd, f"æäº¤ Spark ä»»åŠ¡ (scaleFactor={scale_factor}G)")

# Step 4: å†æ¬¡æ£€æŸ¥æ˜¯å¦ç”Ÿæˆæ•°æ®
print("ğŸ” æ£€æŸ¥ç”Ÿæˆç»“æœæ˜¯å¦å­˜åœ¨...")
verify_cmd = f'ssh {vm_user}@{vm_ip} -i {vm_ssh_key} "du -sh {dataset_dir} || echo ç”Ÿæˆå¤±è´¥"'
subprocess.run(verify_cmd, shell=True)
