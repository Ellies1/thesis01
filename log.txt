 Picked up JAVA_TOOL_OPTIONS: -Djavax.net.ssl.trustStore=/opt/java/openjdk/lib/security/cacerts -Djavax.net.ssl.trustStorePassword=changeit
Picked up JAVA_TOOL_OPTIONS: -Djavax.net.ssl.trustStore=/opt/java/openjdk/lib/security/cacerts -Djavax.net.ssl.trustStorePassword=changeit
Files  local:///opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar from /opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar to /opt/spark/work-dir/./spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar
Files  local:///opt/tpcds/parquet-data-generator_2.12-1.0.jar from /opt/tpcds/parquet-data-generator_2.12-1.0.jar to /opt/spark/work-dir/./parquet-data-generator_2.12-1.0.jar
25/06/13 12:37:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
ðŸ§ª Debug: args length = 3
ðŸ§ª args(0): query
ðŸ§ª args(1): q3-v2.4
ðŸ§ª args(2): 1
ðŸŸ  Mode is: query
25/06/13 12:37:18 INFO HiveConf: Found configuration file null
25/06/13 12:37:18 INFO SparkContext: Running Spark version 3.4.1
25/06/13 12:37:19 INFO ResourceUtils: ==============================================================
25/06/13 12:37:19 INFO ResourceUtils: No custom resources configured for spark.driver.
25/06/13 12:37:19 INFO ResourceUtils: ==============================================================
25/06/13 12:37:19 INFO SparkContext: Submitted application: Data Generator
25/06/13 12:37:19 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memoryOverhead -> name: memoryOverhead, amount: 2048, script: , vendor: , cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 6144, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/06/13 12:37:19 INFO ResourceProfile: Limiting resource is cpus at 3 tasks per executor
25/06/13 12:37:19 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/06/13 12:37:19 INFO SecurityManager: Changing view acls to: spark,zsong
25/06/13 12:37:19 INFO SecurityManager: Changing modify acls to: spark,zsong
25/06/13 12:37:19 INFO SecurityManager: Changing view acls groups to: 
25/06/13 12:37:19 INFO SecurityManager: Changing modify acls groups to: 
25/06/13 12:37:19 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark, zsong; groups with view permissions: EMPTY; users with modify permissions: spark, zsong; groups with modify permissions: EMPTY
25/06/13 12:37:19 INFO Utils: Successfully started service 'sparkDriver' on port 7078.
25/06/13 12:37:19 INFO SparkEnv: Registering MapOutputTracker
25/06/13 12:37:19 INFO SparkEnv: Registering BlockManagerMaster
25/06/13 12:37:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/06/13 12:37:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/06/13 12:37:19 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/06/13 12:37:19 INFO DiskBlockManager: Created local directory at /var/data/spark-541aa465-7b34-44b1-9402-df6038cf28cb/blockmgr-e913354a-f60b-41eb-9815-9226473fa3bb
25/06/13 12:37:19 INFO MemoryStore: MemoryStore started with capacity 2.2 GiB
25/06/13 12:37:19 INFO SparkEnv: Registering OutputCommitCoordinator
25/06/13 12:37:19 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
25/06/13 12:37:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/06/13 12:37:20 INFO SparkContext: Added JAR local:/opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar at file:/opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar with timestamp 1749818238968
25/06/13 12:37:20 INFO SparkContext: Added JAR local:/opt/tpcds/parquet-data-generator_2.12-1.0.jar at file:/opt/tpcds/parquet-data-generator_2.12-1.0.jar with timestamp 1749818238968
25/06/13 12:37:20 WARN SparkContext: The JAR local:///opt/tpcds/parquet-data-generator_2.12-1.0.jar at file:/opt/tpcds/parquet-data-generator_2.12-1.0.jar has been added already. Overwriting of added jar is not supported in the current version.
25/06/13 12:37:20 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
25/06/13 12:37:21 INFO ExecutorPodsAllocator: Going to request 2 executors from Kubernetes for ResourceProfile Id: 0, target: 2, known: 0, sharedSlotFromPendingPods: 2147483647.
25/06/13 12:37:21 INFO ExecutorPodsAllocator: Found 0 reusable PVCs from 0 PVCs
25/06/13 12:37:21 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
25/06/13 12:37:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 7079.
25/06/13 12:37:22 INFO NettyBlockTransferService: Server created on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc 10.244.1.163:7079
25/06/13 12:37:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/06/13 12:37:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc, 7079, None)
25/06/13 12:37:22 INFO BlockManagerMasterEndpoint: Registering block manager parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 with 2.2 GiB RAM, BlockManagerId(driver, parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc, 7079, None)
25/06/13 12:37:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc, 7079, None)
25/06/13 12:37:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc, 7079, None)
25/06/13 12:37:22 INFO BasicExecutorFeatureStep: Decommissioning not enabled, skipping shutdown script
25/06/13 12:37:22 INFO SingleEventLogFileWriter: Logging events to file:/tpcds-data/spark-events/spark-a0671ac1454542dc80a0590d5f7d79a7.inprogress
25/06/13 12:37:26 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.1.164:60298
25/06/13 12:37:26 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: No executor found for 10.244.1.165:43304
25/06/13 12:37:27 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.1.164:60302) with ID 1,  ResourceProfileId 0
25/06/13 12:37:27 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.244.1.165:43308) with ID 2,  ResourceProfileId 0
25/06/13 12:37:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.1.164:41405 with 3.4 GiB RAM, BlockManagerId(1, 10.244.1.164, 41405, None)
25/06/13 12:37:27 INFO KubernetesClusterSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8
25/06/13 12:37:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.244.1.165:46413 with 3.4 GiB RAM, BlockManagerId(2, 10.244.1.165, 46413, None)
25/06/13 12:37:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
25/06/13 12:37:27 INFO SharedState: Warehouse path is 'file:/tpcds-data/hive-warehouse'.
25/06/13 12:37:31 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.9 using Spark classes.
25/06/13 12:37:32 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.9) is file:/tpcds-data/hive-warehouse
25/06/13 12:37:32 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/06/13 12:37:32 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
25/06/13 12:37:32 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
25/06/13 12:37:32 INFO ObjectStore: ObjectStore, initialize called
25/06/13 12:37:32 INFO Persistence: Property spark.hadoop.javax.jdo.option.ConnectionURL unknown - will be ignored
25/06/13 12:37:32 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
25/06/13 12:37:32 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
25/06/13 12:37:35 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
25/06/13 12:37:37 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
25/06/13 12:37:37 INFO ObjectStore: Initialized ObjectStore
25/06/13 12:37:37 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
25/06/13 12:37:37 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@10.244.1.163
25/06/13 12:37:37 INFO HiveMetaStore: Added admin role in metastore
25/06/13 12:37:37 INFO HiveMetaStore: Added public role in metastore
25/06/13 12:37:37 INFO HiveMetaStore: No user is added in admin role, since config is empty
25/06/13 12:37:37 INFO HiveMetaStore: 0: get_database: default
25/06/13 12:37:37 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: default	
25/06/13 12:37:37 INFO HiveMetaStore: 0: get_database: dataset_tpcds_1g
25/06/13 12:37:37 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: dataset_tpcds_1g	
25/06/13 12:37:37 INFO HiveMetaStore: 0: get_database: global_temp
25/06/13 12:37:37 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: global_temp	
25/06/13 12:37:37 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
25/06/13 12:37:37 INFO HiveMetaStore: 0: get_database: dataset_tpcds_1g
25/06/13 12:37:37 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: dataset_tpcds_1g	
Query: q3-v2.4
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_database: dataset_tpcds_1g
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: dataset_tpcds_1g	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_table : db=dataset_tpcds_1g tbl=date_dim
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=date_dim	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_table : db=dataset_tpcds_1g tbl=date_dim
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=date_dim	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_database: dataset_tpcds_1g
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: dataset_tpcds_1g	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_table : db=dataset_tpcds_1g tbl=store_sales
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=store_sales	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_table : db=dataset_tpcds_1g tbl=store_sales
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=store_sales	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_database: dataset_tpcds_1g
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: dataset_tpcds_1g	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_table : db=dataset_tpcds_1g tbl=item
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=item	
25/06/13 12:37:38 INFO HiveMetaStore: 0: get_table : db=dataset_tpcds_1g tbl=item
25/06/13 12:37:38 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=item	
25/06/13 12:37:38 INFO InMemoryFileIndex: It took 47 ms to list leaf files for 1 paths.
25/06/13 12:37:38 INFO InMemoryFileIndex: It took 3 ms to list leaf files for 1 paths.
25/06/13 12:37:39 INFO HiveMetaStore: 1: get_database: dataset_tpcds_1g
25/06/13 12:37:39 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_database: dataset_tpcds_1g	
25/06/13 12:37:39 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
25/06/13 12:37:39 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
25/06/13 12:37:39 INFO HiveMetaStore: 1: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
25/06/13 12:37:39 INFO ObjectStore: ObjectStore, initialize called
25/06/13 12:37:39 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
25/06/13 12:37:39 INFO ObjectStore: Initialized ObjectStore
25/06/13 12:37:39 INFO HiveMetaStore: 1: get_table : db=dataset_tpcds_1g tbl=store_sales
25/06/13 12:37:39 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=store_sales	
25/06/13 12:37:39 INFO HiveMetaStore: 1: get_table : db=dataset_tpcds_1g tbl=store_sales
25/06/13 12:37:39 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_table : db=dataset_tpcds_1g tbl=store_sales	
25/06/13 12:37:39 INFO HiveMetaStore: 1: get_partitions : db=dataset_tpcds_1g tbl=store_sales
25/06/13 12:37:39 INFO audit: ugi=spark	ip=unknown-ip-addr	cmd=get_partitions : db=dataset_tpcds_1g tbl=store_sales	
25/06/13 12:37:41 INFO HadoopFSUtils: Listing leaf files and directories in parallel under 1823 paths. The first several paths are: file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450816, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450817, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450818, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450819, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450820, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450821, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450822, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450823, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450824, file:/tpcds-data/dataset_tpcds_1g/store_sales/ss_sold_date_sk=2450825.
25/06/13 12:37:41 INFO SparkContext: Starting job: show at TPCDS.scala:91
25/06/13 12:37:41 INFO DAGScheduler: Got job 0 (show at TPCDS.scala:91) with 1823 output partitions
25/06/13 12:37:41 INFO DAGScheduler: Final stage: ResultStage 0 (show at TPCDS.scala:91)
25/06/13 12:37:41 INFO DAGScheduler: Parents of final stage: List()
25/06/13 12:37:41 INFO DAGScheduler: Missing parents: List()
25/06/13 12:37:41 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at show at TPCDS.scala:91), which has no missing parents
25/06/13 12:37:41 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 106.0 KiB, free 2.2 GiB)
25/06/13 12:37:41 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 38.4 KiB, free 2.2 GiB)
25/06/13 12:37:41 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 38.4 KiB, free: 2.2 GiB)
25/06/13 12:37:41 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
25/06/13 12:37:41 INFO DAGScheduler: Submitting 1823 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at show at TPCDS.scala:91) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
25/06/13 12:37:41 INFO TaskSchedulerImpl: Adding task set 0.0 with 1823 tasks resource profile 0
25/06/13 12:37:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (10.244.1.164, executor 1, partition 0, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:41 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1) (10.244.1.165, executor 2, partition 1, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:41 INFO TaskSetManager: Starting task 2.0 in stage 0.0 (TID 2) (10.244.1.164, executor 1, partition 2, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:41 INFO TaskSetManager: Starting task 3.0 in stage 0.0 (TID 3) (10.244.1.165, executor 2, partition 3, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:41 INFO TaskSetManager: Starting task 4.0 in stage 0.0 (TID 4) (10.244.1.164, executor 1, partition 4, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:41 INFO TaskSetManager: Starting task 5.0 in stage 0.0 (TID 5) (10.244.1.165, executor 2, partition 5, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.244.1.165:46413 (size: 38.4 KiB, free: 3.4 GiB)
25/06/13 12:37:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.244.1.164:41405 (size: 38.4 KiB, free: 3.4 GiB)
25/06/13 12:37:43 INFO TaskSetManager: Starting task 6.0 in stage 0.0 (TID 6) (10.244.1.165, executor 2, partition 6, PROCESS_LOCAL, 7450 bytes) 
25/06/13 12:37:43 INFO TaskSetManager: Starting task 7.0 in stage 0.0 (TID 7) (10.244.1.165, executor 2, partition 7, PROCESS_LOCAL, 7450 bytes) 

25/06/13 12:37:49 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
25/06/13 12:37:49 INFO DAGScheduler: ResultStage 0 (show at TPCDS.scala:91) finished in 8.247 s
25/06/13 12:37:49 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
25/06/13 12:37:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
25/06/13 12:37:50 INFO DAGScheduler: Job 0 finished: show at TPCDS.scala:91, took 8.400570 s
25/06/13 12:37:50 INFO InMemoryFileIndex: It took 9035 ms to list leaf files for 1823 paths.
25/06/13 12:37:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(d_moy),EqualTo(d_moy,11),IsNotNull(d_date_sk)
25/06/13 12:37:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(d_moy#11),(d_moy#11 = 11),isnotnull(d_date_sk#3)
25/06/13 12:37:50 INFO DataSourceStrategy: Pruning directories with: isnotnull(ss_sold_date_sk#53),dynamicpruning#96 [ss_sold_date_sk#53]
25/06/13 12:37:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(ss_item_sk)
25/06/13 12:37:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(ss_item_sk#32)
25/06/13 12:37:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(i_manufact_id),EqualTo(i_manufact_id,128),IsNotNull(i_item_sk)
25/06/13 12:37:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(i_manufact_id#67),(i_manufact_id#67 = 128),isnotnull(i_item_sk#54)
25/06/13 12:37:50 INFO FileSourceStrategy: Pushed Filters: IsNotNull(d_moy),EqualTo(d_moy,11),IsNotNull(d_date_sk)
25/06/13 12:37:50 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(d_moy#11),(d_moy#11 = 11),isnotnull(d_date_sk#3)
25/06/13 12:37:50 INFO CodeGenerator: Code generated in 316.374229 ms
25/06/13 12:37:51 INFO CodeGenerator: Code generated in 322.145136 ms
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 206.4 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 206.5 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 36.5 KiB, free: 2.2 GiB)
25/06/13 12:37:51 INFO SparkContext: Created broadcast 1 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 36.6 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 36.6 KiB, free: 2.2 GiB)
25/06/13 12:37:51 INFO SparkContext: Created broadcast 2 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
25/06/13 12:37:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/06/13 12:37:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.
25/06/13 12:37:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at <unknown>:0
25/06/13 12:37:51 INFO DAGScheduler: Got job 1 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) with 1 output partitions
25/06/13 12:37:51 INFO DAGScheduler: Final stage: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0)
25/06/13 12:37:51 INFO DAGScheduler: Parents of final stage: List()
25/06/13 12:37:51 INFO DAGScheduler: Missing parents: List()
25/06/13 12:37:51 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[10] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0), which has no missing parents
25/06/13 12:37:51 INFO SparkContext: Starting job: $anonfun$withThreadLocalCaptured$1 at <unknown>:0
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 15.2 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 6.5 KiB, free: 2.2 GiB)
25/06/13 12:37:51 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
25/06/13 12:37:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[10] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/06/13 12:37:51 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
25/06/13 12:37:51 INFO DAGScheduler: Got job 2 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) with 1 output partitions
25/06/13 12:37:51 INFO DAGScheduler: Final stage: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0)
25/06/13 12:37:51 INFO DAGScheduler: Parents of final stage: List()
25/06/13 12:37:51 INFO DAGScheduler: Missing parents: List()
25/06/13 12:37:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0), which has no missing parents
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 14.6 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 6.3 KiB, free 2.2 GiB)
25/06/13 12:37:51 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1823) (10.244.1.164, executor 1, partition 0, PROCESS_LOCAL, 8007 bytes) 
25/06/13 12:37:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 6.3 KiB, free: 2.2 GiB)
25/06/13 12:37:51 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
25/06/13 12:37:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[9] at $anonfun$withThreadLocalCaptured$1 at <unknown>:0) (first 15 tasks are for partitions Vector(0))
25/06/13 12:37:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
25/06/13 12:37:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1824) (10.244.1.164, executor 1, partition 0, PROCESS_LOCAL, 8011 bytes) 
25/06/13 12:37:51 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.244.1.164:41405 (size: 6.5 KiB, free: 3.4 GiB)
25/06/13 12:37:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.244.1.164:41405 (size: 6.3 KiB, free: 3.4 GiB)
25/06/13 12:37:52 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.244.1.164:41405 (size: 36.6 KiB, free: 3.4 GiB)
25/06/13 12:37:52 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.244.1.164:41405 (size: 36.5 KiB, free: 3.4 GiB)
25/06/13 12:37:53 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1823) in 2504 ms on 10.244.1.164 (executor 1) (1/1)
25/06/13 12:37:53 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
25/06/13 12:37:53 INFO DAGScheduler: ResultStage 1 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) finished in 2.518 s
25/06/13 12:37:53 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
25/06/13 12:37:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
25/06/13 12:37:53 INFO DAGScheduler: Job 1 finished: $anonfun$withThreadLocalCaptured$1 at <unknown>:0, took 2.527688 s
25/06/13 12:37:53 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1824) in 2510 ms on 10.244.1.164 (executor 1) (1/1)
25/06/13 12:37:53 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
25/06/13 12:37:53 INFO DAGScheduler: ResultStage 2 ($anonfun$withThreadLocalCaptured$1 at <unknown>:0) finished in 2.546 s
25/06/13 12:37:53 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
25/06/13 12:37:53 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
25/06/13 12:37:53 INFO DAGScheduler: Job 2 finished: $anonfun$withThreadLocalCaptured$1 at <unknown>:0, took 2.564562 s
25/06/13 12:37:53 INFO CodeGenerator: Code generated in 22.370396 ms
25/06/13 12:37:53 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1417.0 B, free 2.2 GiB)
25/06/13 12:37:53 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 1417.0 B, free: 2.2 GiB)
25/06/13 12:37:53 INFO SparkContext: Created broadcast 5 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
25/06/13 12:37:53 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 91.9 KiB, free 2.2 GiB)
25/06/13 12:37:53 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 91.9 KiB, free: 2.2 GiB)
25/06/13 12:37:53 INFO SparkContext: Created broadcast 6 from $anonfun$withThreadLocalCaptured$1 at <unknown>:0
25/06/13 12:37:53 INFO DataSourceStrategy: Pruning directories with: isnotnull(ss_sold_date_sk#53),dynamicpruning#96 [ss_sold_date_sk#53]
25/06/13 12:37:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(ss_item_sk)
25/06/13 12:37:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(ss_item_sk#32)
25/06/13 12:37:53 INFO DataSourceStrategy: Pruning directories with: isnotnull(ss_sold_date_sk#53),dynamicpruning#96 [ss_sold_date_sk#53]
25/06/13 12:37:53 INFO FileSourceStrategy: Pushed Filters: IsNotNull(ss_item_sk)
25/06/13 12:37:53 INFO FileSourceStrategy: Post-Scan Filters: isnotnull(ss_item_sk#32)
25/06/13 12:37:54 INFO CodeGenerator: Code generated in 20.132687 ms
25/06/13 12:37:54 INFO CodeGenerator: Code generated in 121.918898 ms
25/06/13 12:37:54 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 206.3 KiB, free 2.2 GiB)
25/06/13 12:37:54 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 36.5 KiB, free 2.2 GiB)
25/06/13 12:37:54 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 36.5 KiB, free: 2.2 GiB)
25/06/13 12:37:54 INFO SparkContext: Created broadcast 7 from show at TPCDS.scala:91
25/06/13 12:37:54 INFO CodeGenerator: Code generated in 29.220164 ms
25/06/13 12:37:54 INFO InMemoryFileIndex: Selected 1823 partitions out of 1823, pruned 0.0% partitions.
25/06/13 12:37:54 INFO FileSourceScanExec: Planning scan with bin packing, max size: 108974185 bytes, open cost is considered as scanning 4194304 bytes.
25/06/13 12:37:54 INFO DAGScheduler: Registering RDD 14 (show at TPCDS.scala:91) as input to shuffle 0
25/06/13 12:37:54 INFO DAGScheduler: Got map stage job 3 (show at TPCDS.scala:91) with 6 output partitions
25/06/13 12:37:54 INFO DAGScheduler: Final stage: ShuffleMapStage 3 (show at TPCDS.scala:91)
25/06/13 12:37:54 INFO DAGScheduler: Parents of final stage: List()
25/06/13 12:37:54 INFO DAGScheduler: Missing parents: List()
25/06/13 12:37:54 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[14] at show at TPCDS.scala:91), which has no missing parents
25/06/13 12:37:54 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 64.4 KiB, free 2.2 GiB)
25/06/13 12:37:54 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 27.3 KiB, free 2.2 GiB)
25/06/13 12:37:54 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 27.3 KiB, free: 2.2 GiB)
25/06/13 12:37:54 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
25/06/13 12:37:54 INFO DAGScheduler: Submitting 6 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[14] at show at TPCDS.scala:91) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5))
25/06/13 12:37:54 INFO TaskSchedulerImpl: Adding task set 3.0 with 6 tasks resource profile 0
25/06/13 12:37:54 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 1825) (10.244.1.164, executor 1, partition 0, PROCESS_LOCAL, 13075 bytes) 

25/06/13 12:37:57 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
25/06/13 12:37:57 INFO DAGScheduler: ShuffleMapStage 3 (show at TPCDS.scala:91) finished in 3.233 s
25/06/13 12:37:57 INFO DAGScheduler: looking for newly runnable stages
25/06/13 12:37:57 INFO DAGScheduler: running: Set()
25/06/13 12:37:57 INFO DAGScheduler: waiting: Set()
25/06/13 12:37:57 INFO DAGScheduler: failed: Set()
25/06/13 12:37:57 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576
25/06/13 12:37:57 INFO CodeGenerator: Code generated in 21.373139 ms
25/06/13 12:37:57 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.
25/06/13 12:37:57 INFO CodeGenerator: Code generated in 26.773194 ms
25/06/13 12:37:57 INFO SparkContext: Starting job: show at TPCDS.scala:91
25/06/13 12:37:57 INFO DAGScheduler: Got job 4 (show at TPCDS.scala:91) with 1 output partitions
25/06/13 12:37:57 INFO DAGScheduler: Final stage: ResultStage 5 (show at TPCDS.scala:91)
25/06/13 12:37:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)
25/06/13 12:37:57 INFO DAGScheduler: Missing parents: List()
25/06/13 12:37:57 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at show at TPCDS.scala:91), which has no missing parents
25/06/13 12:37:58 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 64.4 KiB, free 2.2 GiB)
25/06/13 12:37:58 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 2.2 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 (size: 27.7 KiB, free: 2.2 GiB)
25/06/13 12:37:58 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
25/06/13 12:37:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at show at TPCDS.scala:91) (first 15 tasks are for partitions Vector(0))
25/06/13 12:37:58 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
25/06/13 12:37:58 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 1831) (10.244.1.165, executor 2, partition 0, NODE_LOCAL, 7382 bytes) 
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 10.244.1.164:41405 in memory (size: 6.5 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_3_piece0 on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 in memory (size: 6.5 KiB, free: 2.2 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.244.1.165:46413 (size: 27.7 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.244.1.164:41405 in memory (size: 27.3 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 in memory (size: 27.3 KiB, free: 2.2 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 10.244.1.165:46413 in memory (size: 27.3 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_4_piece0 on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 in memory (size: 6.3 KiB, free: 2.2 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 10.244.1.164:41405 in memory (size: 6.3 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.244.1.164:41405 in memory (size: 38.4 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.244.1.165:46413 in memory (size: 38.4 KiB, free: 3.4 GiB)
25/06/13 12:37:58 INFO BlockManagerInfo: Removed broadcast_0_piece0 on parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:7079 in memory (size: 38.4 KiB, free: 2.2 GiB)
25/06/13 12:37:58 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.244.1.165:43308
25/06/13 12:37:58 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 1831) in 444 ms on 10.244.1.165 (executor 2) (1/1)
25/06/13 12:37:58 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
25/06/13 12:37:58 INFO DAGScheduler: ResultStage 5 (show at TPCDS.scala:91) finished in 0.520 s
25/06/13 12:37:58 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
25/06/13 12:37:58 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
25/06/13 12:37:58 INFO DAGScheduler: Job 4 finished: show at TPCDS.scala:91, took 0.559087 s
25/06/13 12:37:58 INFO CodeGenerator: Code generated in 21.51831 ms
25/06/13 12:37:58 INFO CodeGenerator: Code generated in 14.441813 ms
+------+--------+----------------+--------+
|d_year|brand_id|           brand| sum_agg|
+------+--------+----------------+--------+
|  1998| 1004001|edu packamalg #1|65716.37|
+------+--------+----------------+--------+
only showing top 1 row

   Took: 20665 ms
------------------------------------------------------------------
Ran 1 out of 1
ArrayBuffer("q3-v2.4")
25/06/13 12:37:58 INFO SparkContext: SparkContext is stopping with exitCode 0.
25/06/13 12:37:58 INFO SparkUI: Stopped Spark web UI at http://parquetgenerator-3ccd5897694b4fab-driver-svc.default.svc:4040
25/06/13 12:37:58 INFO KubernetesClusterSchedulerBackend: Shutting down all executors
25/06/13 12:37:58 INFO KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Asking each executor to shut down
25/06/13 12:37:58 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed.
25/06/13 12:37:58 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
25/06/13 12:37:58 INFO MemoryStore: MemoryStore cleared
25/06/13 12:37:58 INFO BlockManager: BlockManager stopped
25/06/13 12:37:58 INFO BlockManagerMaster: BlockManagerMaster stopped
25/06/13 12:37:58 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
25/06/13 12:37:58 INFO SparkContext: Successfully stopped SparkContext
25/06/13 12:37:58 INFO ShutdownHookManager: Shutdown hook called
25/06/13 12:37:58 INFO ShutdownHookManager: Deleting directory /var/data/spark-541aa465-7b34-44b1-9402-df6038cf28cb/spark-ccdb3725-7d3d-4deb-ac24-0f125ef0da1f
25/06/13 12:37:58 INFO ShutdownHookManager: Deleting directory /tmp/spark-0fcb1d72-45a6-4f05-9807-c34aa0290df8