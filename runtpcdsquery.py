import subprocess

# === ç”¨æˆ·å¯ä¿®æ”¹é…ç½® ===
scale_factor = 1
query_file = "q3-v2.4"  # å®¹å™¨å†…çš„æŸ¥è¯¢è„šæœ¬è·¯å¾„

# === å›ºå®šè®¾ç½® ===
vm_ip = "192.168.166.3"
vm_user = "cloud0_zsong"
vm_ssh_key = "/home/zsong/.ssh/id_rsa_continuum"
vm_data_dir = "/tpcds-data"
dataset_dir = f"{vm_data_dir}/dataset_tpcds_{scale_factor}g"

def run(command, desc, exit_on_fail=True, shell=False):
    print(f"ğŸ”§ {desc}...")
    try:
        subprocess.run(command, check=True, shell=shell)
        print(f"âœ… {desc} æˆåŠŸ\n")
    except subprocess.CalledProcessError as e:
        print(f"âŒ {desc} å¤±è´¥: {e}\n")
        if exit_on_fail:
            exit(1)

# Step 1: æ£€æŸ¥å¹¶æ¸…ç†æ—§æ•°æ®ï¼ˆå¦‚æœ‰ï¼‰
check_dataset_cmd = f'ssh {vm_user}@{vm_ip} -i {vm_ssh_key} "test -d {dataset_dir} && echo FOUND || echo NOT_FOUND"'
check_output = subprocess.getoutput(check_dataset_cmd)
if "FOUND" in check_output:
    print("âš ï¸ æ£€æµ‹åˆ°æ—§æ•°æ®ï¼Œå³å°†æ¸…ç†...\n")
    cleanup_cmd = [
        "ssh", f"{vm_user}@{vm_ip}", "-i", vm_ssh_key,
        f"sudo rm -rf {vm_data_dir}"
    ]
    run(cleanup_cmd, f"åˆ é™¤æ—§ç›®å½• {vm_data_dir}")

# Step 2: åˆ›å»º VM ä¸Šçš„æ•°æ®ç›®å½•
mkdir_cmd = [
    "ssh", f"{vm_user}@{vm_ip}", "-i", vm_ssh_key,
    f"sudo mkdir -p {vm_data_dir} && sudo chmod -R 777 {vm_data_dir}"
]
run(mkdir_cmd, f"åˆ›å»º VM ä¸Šçš„ç›®å½• {vm_data_dir}")

# Step 3: è¿è¡Œ datagen
spark_submit_base = [
    "~/continuum/spark-3.4.4-bin-hadoop3/bin/spark-submit",
    "--class", "ParquetGenerator",
    "--master", "k8s://https://192.168.166.2:6443",
    "--deploy-mode", "cluster",
    "--conf", "spark.kubernetes.container.image=elliesgood00/tpcds-image:v16",
    "--conf", "spark.kubernetes.authenticate.driver.serviceAccountName=spark",
    "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.mount.path=/tpcds-data",
    "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.options.path=/tpcds-data",
    "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.options.readOnly=false",
    "--conf", "spark.kubernetes.executor.volumes.hostPath.tpcds.options.type=Directory",
    "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.mount.path=/tpcds-data",
    "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.options.path=/tpcds-data",
    "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.options.readOnly=false",
    "--conf", "spark.kubernetes.driver.volumes.hostPath.tpcds.options.type=Directory",
    "--conf", "spark.sql.catalogImplementation=hive",
    "--conf", "spark.hadoop.javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/tpcds-data/metastore_db;create=true",
    "--conf", "spark.sql.warehouse.dir=/tpcds-data/hive-warehouse",
    "--conf", "spark.executor.memory=6g",
    "--conf", "spark.executor.memoryOverhead=2g",
    "--conf", "spark.driver.memory=4g",
    "--conf", "spark.driver.memoryOverhead=1g",
    "--jars", "local:///opt/tpcds/lib/spark-sql-perf_2.12-0.5.1-SNAPSHOT.jar",
    "local:///opt/tpcds/parquet-data-generator_2.12-1.0.jar"
]
spark_submit_base[0] = spark_submit_base[0].replace("~", subprocess.getoutput("echo $HOME"))

run(spark_submit_base + ["datagen", vm_data_dir, "/opt/tpcds-kit/tools", str(scale_factor)],
    f"æäº¤ Spark æ•°æ®ç”Ÿæˆä»»åŠ¡ (scaleFactor={scale_factor}g)")

# Step 4: æ‰§è¡Œ metagen å»ºè¡¨
print("ğŸš€ Metagen command:", " ".join(spark_submit_base + ["metagen", vm_data_dir, str(scale_factor)]))
run(spark_submit_base + ["metagen", vm_data_dir, str(scale_factor)],
    "æäº¤ Spark å…ƒæ•°æ®ä»»åŠ¡ (metagen)")

# Step 5: æ‰§è¡Œ query
query_cmd = spark_submit_base + ["query", query_file, str(scale_factor)]
print("ğŸš€ Query command:", " ".join(query_cmd))
run(spark_submit_base + ["query", query_file, str(scale_factor)],
    f"æäº¤ Spark æŸ¥è¯¢ä»»åŠ¡ ({query_file})")

